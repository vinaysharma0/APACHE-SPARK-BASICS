{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prepared-draft",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "weekly-bulgarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "broken-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = spark.read.text(\"README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "intense-grammar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "precious-extra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='# APACHE-SPARK-BASICS'),\n",
       " Row(value=''),\n",
       " Row(value=''),\n",
       " Row(value='Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fundamental-congo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(value='# APACHE-SPARK-BASICS')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "alike-measure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='# APACHE-SPARK-BASICS'), Row(value=''), Row(value='')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "british-minnesota",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.filter(file.value.contains(\"Spark\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cognitive-syntax",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.filter(file.value.contains(\"Spark\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "homeless-orange",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "greek-suite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(word='#'),\n",
       " Row(word='APACHE-SPARK-BASICS'),\n",
       " Row(word=''),\n",
       " Row(word=''),\n",
       " Row(word='Apache'),\n",
       " Row(word='Spark'),\n",
       " Row(word='is'),\n",
       " Row(word='a'),\n",
       " Row(word='unified'),\n",
       " Row(word='analytics'),\n",
       " Row(word='engine'),\n",
       " Row(word='for'),\n",
       " Row(word='large-scale'),\n",
       " Row(word='data'),\n",
       " Row(word='processing.'),\n",
       " Row(word='It'),\n",
       " Row(word='provides'),\n",
       " Row(word='high-level'),\n",
       " Row(word='APIs'),\n",
       " Row(word='in'),\n",
       " Row(word='Java,'),\n",
       " Row(word='Scala,'),\n",
       " Row(word='Python'),\n",
       " Row(word='and'),\n",
       " Row(word='R,'),\n",
       " Row(word='and'),\n",
       " Row(word='an'),\n",
       " Row(word='optimized'),\n",
       " Row(word='engine'),\n",
       " Row(word='that'),\n",
       " Row(word='supports'),\n",
       " Row(word='general'),\n",
       " Row(word='execution'),\n",
       " Row(word='graphs.'),\n",
       " Row(word='It'),\n",
       " Row(word='also'),\n",
       " Row(word='supports'),\n",
       " Row(word='a'),\n",
       " Row(word='rich'),\n",
       " Row(word='set'),\n",
       " Row(word='of'),\n",
       " Row(word='higher-level'),\n",
       " Row(word='tools'),\n",
       " Row(word='including'),\n",
       " Row(word='Spark'),\n",
       " Row(word='SQL'),\n",
       " Row(word='for'),\n",
       " Row(word='SQL'),\n",
       " Row(word='and'),\n",
       " Row(word='structured'),\n",
       " Row(word='data'),\n",
       " Row(word='processing,'),\n",
       " Row(word='MLlib'),\n",
       " Row(word='for'),\n",
       " Row(word='machine'),\n",
       " Row(word='learning,'),\n",
       " Row(word='GraphX'),\n",
       " Row(word='for'),\n",
       " Row(word='graph'),\n",
       " Row(word='processing,'),\n",
       " Row(word='and'),\n",
       " Row(word='Structured'),\n",
       " Row(word='Streaming'),\n",
       " Row(word='for'),\n",
       " Row(word='incremental'),\n",
       " Row(word='computation'),\n",
       " Row(word='and'),\n",
       " Row(word='stream'),\n",
       " Row(word='processing.')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting every word\n",
    "file.select(explode(split(file.value, \"\\s+\")).alias(\"word\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "stunning-plumbing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|               word|count|\n",
      "+-------------------+-----+\n",
      "|            graphs.|    1|\n",
      "|               rich|    1|\n",
      "|                set|    1|\n",
      "|             stream|    1|\n",
      "|              MLlib|    1|\n",
      "|                for|    5|\n",
      "|           provides|    1|\n",
      "|                 in|    1|\n",
      "|            machine|    1|\n",
      "|          optimized|    1|\n",
      "|                 R,|    1|\n",
      "|              graph|    1|\n",
      "|            general|    1|\n",
      "|APACHE-SPARK-BASICS|    1|\n",
      "|          learning,|    1|\n",
      "|             Apache|    1|\n",
      "|                 is|    1|\n",
      "|          execution|    1|\n",
      "|               data|    2|\n",
      "|        processing.|    2|\n",
      "+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#file.select(explode(split(file.value, \"\\s+\")).alias(\"word\")).groupBy(\"word\").count().collect()\n",
    "\n",
    "word_count = file.select(explode(split(file.value, \"\\s+\")).alias(\"word\")).groupBy(\"word\").count()\n",
    "word_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "unable-korea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|max(numWords)|\n",
      "+-------------+\n",
      "|           65|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_word_count = file.select(size(split(file.value, \"\\s+\")).alias(\"numWords\")).agg(max(col(\"numWords\")))\n",
    "max_word_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "respective-biography",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string, count: bigint]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "disciplinary-crawford",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-hartford",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
